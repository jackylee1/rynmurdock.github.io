# Single-Network CCNs

![A CCN font!](/images/typeface.png)

>After reading about [Dimensions of Dialogue](https://www.joelsimon.net/dimensions-of-dialogue.html) (the seminal work on CCNs)
and [GlyphNet](https://github.com/noahtren/GlyphNet), 
>I decided to investigate the idea of two artificial neural networks (NNs) communicating over a noisy channel.
>The idea is to have an arbitrary number of classes that serve as content to be communicated effectively between an observer and speaker,
all with the chance of some of the information in the communication to be lost.

"Communication" does not make sense in the context of only one agent. The premise of [CCNs](https://rynmurdock.github.io/2020/02/05/CCN.html)
is that two networks are challenged to
cooperate against noise/interference and in the process produce some representation that is interesting --- or at least 
good at withstanding noise. So what could a single-network CCN even be?

Let's think of CCNs from a different perspective: instead of conceptualizing their output as just "noise-robust," let's consider
what that means. Each symbol should be recognizably different from each other symbol so the networks know which class is being 
communicated.

Each symbol should also have as little redundancy as possible so it isn't drowned out by the noise/interference being 
applied to it. This idea can be applied in a variety of ways. Here it is ultimately operationalized as follows: if an image is
not redundant, it should (at least look to) contain a lot of information. If it contains a lot of information, it will be difficult
to compress.

With those properties in mind (discriminability and compressability), we are ready to build a single-network CCN. 
The first step is
to decide on a loss function. How can you determine discriminability? Well, since we're applying this to every image simultaneously,
it must be pairwise. And because I didn't want to use perceptual loss (although this would be a super cool thing to try), it should
probably simply use pixel-distance. So that's the first step: find the mean-absolute difference between every pixel in every image.
I call this pair-wise MAE.

How do we easily measure how compressable something is? Zipping the results of each output probably 
isn't going to be efficient, but what
if we measured the explained variance of the first principle component? (i.e., run PCA and ask how much variance it can explain in 
just one dimension.) That would work, but TensorFlow has an even easier answer: `tf.linalg.svd()`. I ultimately just run 
the builtin singular value decomposition function and use the mean of all the largest singular values for each image.

With some tweaking and weighting, the results look quite similar to a traditional CCN. They also are much quicker to generate;
using a TPU is trivial with just one network, so I did in the colab notebook, which I can share if there is interest. I also
decided to actually create a web-font that anyone can use. I have the TTF and WOOF files, and the image above is an example
of what the typeface looks like. 

Finally, I added self-attention. The results are somewhat more suggestive of the alien language you might expect from an
artificial neural network ;)

![Alien output](/images/oasllasdkl.jpeg)



